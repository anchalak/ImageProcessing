# -*- coding: utf-8 -*-
"""Anomaly Detection in Images

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JA6a8FLSYc2sFK7RlBOsoWwC6AkPIypP

# **Anomaly Detection in MNIST Images**

Here I use three different models to find anomalous digits.

##**Autoencoder**

In this project, we implemented an anomaly detection system using autoencoders on the MNIST dataset, which consists of grayscale images of handwritten digits from 0 to 9. The goal is to detect anomalous digits (digits 7 to 9) from normal digits (digits 0 to 4) using an unsupervised learning approach.
"""

#Importing necessary packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from keras.datasets import mnist
from keras.models import Model, Sequential
from keras.layers import Input, Dense
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import fetch_openml
from sklearn.ensemble import IsolationForest

# Load the MNIST dataset. Here I will be using MNIST datasets since this size of dataset can be handled by my personal devices.
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Flatten the images and normalize pixel values to [0, 1]
X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255.0

anomaly_digits = [7, 8, 9]
y_anomaly = np.array([1 if digit in anomaly_digits else 0 for digit in y_test])

# Display the first 25 images from the training dataset
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')
    plt.title(f"Digit {y_train[i]}")
    plt.axis('off')

plt.tight_layout()
plt.show()

"""An autoencoder is an unsupervised artificial neural network used for dimensionality reduction and feature learning. It consists of an encoder that compresses the input data into a lower-dimensional representation (latent space) and a decoder that reconstructs the original data from this representation. During training, the autoencoder learns to minimize the reconstruction loss, encouraging it to capture essential features and patterns in the input data. The compressed representation in the latent space can be utilized for various tasks like anomaly detection, denoising, and feature extraction"""

anomaly_digits = [7, 8, 9]
y_anomaly = np.array([1 if digit in anomaly_digits else 0 for digit in y_test])

# Build the Autoencoder model
input_dim = X_train.shape[1]
encoding_dim = 32  # Adjust the encoding dimension as needed

encoder = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dense(encoding_dim, activation='relu')
])

decoder = Sequential([
    Dense(128, activation='relu', input_shape=(encoding_dim,)),
    Dense(input_dim, activation='sigmoid')
])

autoencoder = Sequential([encoder, decoder])
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, validation_data=(X_test, X_test))

# Make predictions on the test set
y_pred = autoencoder.predict(X_test)

# Compute reconstruction error (MSE) for each instance
mse = np.mean(np.square(X_test - y_pred), axis=1)

threshold = np.percentile(mse, 95)
y_pred_class = np.where(mse > threshold, 1, 0)

# Display some of the detected anomalies (outliers) and their original digits
anomalous_indices = np.where(y_pred_class == 1)[0][:10]  # Get the indices of the first 10 anomalies
plt.figure(figsize=(12, 6))
for i, idx in enumerate(anomalous_indices):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')
    plt.title(f"Anomaly {i + 1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

"""## **Isolation Forest Model**

The key idea behind Isolation Forest is to isolate anomalies (outliers) by constructing a tree-based partitioning of the data space. The algorithm is based on the intuition that anomalies are rare instances that can be easily separated from the majority of the data.
"""

# Load the MNIST dataset
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data.astype('float32') / 255.0
y = mnist.target.astype('int')

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Isolation Forest model
clf = IsolationForest(contamination=0.05, random_state=42)  # Contamination is the proportion of anomalies expected in the data
clf.fit(X_train)

# Predict anomaly scores for the test set
y_pred = clf.predict(X_test)
y_pred = np.where(y_pred == 1, 0, 1)  # Convert predictions to binary (0: inliers, 1: outliers)

# Evaluate the model
n_normal = len(y_test[y_test == 0])
n_anomaly = len(y_test[y_test == 1])
n_detected = len(y_test[y_test == y_pred])
n_correctly_detected = len(y_test[(y_test == 1) & (y_pred == 1)])

print(f"Number of normal instances: {n_normal}")
print(f"Number of anomalies: {n_anomaly}")
print(f"Number of detected instances: {n_detected}")
print(f"Number of correctly detected anomalies: {n_correctly_detected}")

"""## **Local Outlier Factor Model**

Local Outlier Factor (LOF) is another popular algorithm for anomaly detection, and it takes a different approach compared to Isolation Forest. LOF is designed to identify local anomalies, meaning instances that are outliers in their local neighborhoods rather than being global anomalies across the entire dataset.
"""

# Load the MNIST dataset
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data.astype('float32') / 255.0  # Normalize pixel values to [0, 1]
y = mnist.target.astype('int')

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Local Outlier Factor model
clf = LocalOutlierFactor(n_neighbors=20, contamination=0.05)  # Contamination is the proportion of anomalies expected in the data
y_pred = clf.fit_predict(X_test)

# Convert predictions to binary (0: inliers, 1: outliers)
y_pred = np.where(y_pred == -1, 1, 0)

# Evaluate the model
n_normal = len(y_test[y_test == 0])
n_anomaly = len(y_test[y_test == 1])
n_detected = len(y_test[y_test == y_pred])
n_correctly_detected = len(y_test[(y_test == 1) & (y_pred == 1)])

print(f"Number of normal instances: {n_normal}")
print(f"Number of anomalies: {n_anomaly}")
print(f"Number of detected instances: {n_detected}")
print(f"Number of correctly detected anomalies: {n_correctly_detected}")

# Plot some of the anomalous digits (outliers) if there are at least 10
anomalous_indices = np.where(y_pred == 1)[0]
n_anomalous_to_plot = min(len(anomalous_indices), 10)

plt.figure(figsize=(12, 6))
for i, idx in enumerate(anomalous_indices[:n_anomalous_to_plot]):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_test.to_numpy()[idx].reshape(28, 28), cmap='gray')  # Convert X_test to NumPy array and reshape
    plt.title(f"Anomaly {i + 1}")
    plt.axis('off')

plt.tight_layout()
plt.show()

